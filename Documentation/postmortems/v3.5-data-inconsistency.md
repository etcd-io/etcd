# v3.5 data inconsistency postmortem

|         |            |
| ------- | ---------- |
| Authors | serathius@ |
| Date    | 2022-04-20 |
| Status  | published  |

## Summary

|         |                                                                                                                                                                                                                                      |
| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Summary | Code refactor in v3.5.0 resulted in the consistent index not being saved atomically. An independent crash could lead to committed transactions not being reflected on all members.                                                   |
| Impact  | No users reported problems in production as triggering the issue required frequent crashes; however, the issue was critical enough to motivate a public statement. The main impact comes from losing user trust in etcd reliability. |

## Background

etcd v3 state is preserved on disk in two forms: the write-ahead log (WAL) and the database state (DB).
etcd v3.5 also still maintains v2 state; however, it's deprecated and not relevant to the issue in this postmortem.

WAL stores the history of changes for etcd state and the database represents state at a single point.
To know which point of history the database represents, it stores the consistent index (CI).
It's a metadata field that points to the last entry in the WAL that it has seen.

When etcd updates the database state, it replays entries from the WAL and updates the consistent index to point to the new entry.
This operation must be [atomic](https://en.wikipedia.org/wiki/Atomic_commit).
A partial failure would mean that the database and WAL would no longer match, so some entries would either be skipped (if only CI is updated) or executed twice (if only changes are applied).
This is especially important for a distributed system like etcd, where multiple cluster members each apply WAL entries to their database.
Correctness of the system depends on the assumption that every member of the cluster, while replaying WAL entries, will reach the same state.

## Root cause

To simplify managing the consistency index, etcd introduced backend hooks in https://github.com/etcd-io/etcd/pull/12855.
The goal was to ensure that the consistency index is always updated by automatically triggering an update during commit.
The implementation was as follows: before applying the WAL entries, etcd updated the in-memory value of the consistent index.
As part of the transaction commit process, a database hook would read the value of the consistent index and store it to the database.

The problem is that the in-memory value of the consistent index is shared, and there might be other in-flight transactions apart from the serial WAL apply flow.
Consider the scenario:

1. The etcd server starts an apply workflow and sets a new consistent index value.
2. A periodic commit is triggered, and it executes the backend hook and saves the consistent index from the apply workflow.
3. The etcd server finishes the apply workflow, saves new changes and saves the same value of the consistent index again.

Between the second and third points there is a small window where the consistent index is increased without applying the corresponding entry from the WAL.

## Trigger

If etcd crashed after the consistent index is saved but before the apply workflow finished, it would lead to data inconsistency.
When recovering the data, etcd would skip executing changes from the failed apply workflow, assuming they had already been executed.

This is consistent with issue reports and the code used to reproduce the issue where the trigger was etcd crashing under high request load.
Etcd v3.5.0 was released with a bug (https://github.com/etcd-io/etcd/pull/13505) that could cause etcd to crash; this was fixed in v3.5.1.
Additionally, reports described etcd running under high memory pressure, causing it to OOM from time to time.
To reproduce, we ran etcd under high stress and randomly killed one of the members using SIGKILL (an immediate, non-recoverable process death).

## Detection

For a single-member cluster it is undetectable.
There is no mechanism or tool for verifying that the database state matches the WAL.

In a cluster with multiple members it would mean that one of the members that crashed will be missing changes from the failed apply workflow.
This means that it will have a different database state and will return a different hash via the `HashKV` gRPC call.

There is an automatic mechanism to detect data inconsistency.
It can be executed during etcd start via `--experimental-initial-corrupt-check` and periodically via `--experimental-corrupt-check-time`.
Both checks, however, have a flaw: they depend on the `HashKV` gRPC method, which might fail, causing the check to pass incorrectly.

In a multi-member etcd cluster, each member can run with different performance and be at a different stage of applying the WAL log.
Comparing database hashes between multiple etcd members requires all hashes to be calculated at the same change.
This is done by requesting a hash for the same `revision` (version of the key-value store).
However, it will not work if the provided revision is not available on all members.
This can happen on very slow members, or in cases where corruption has led to diverging revision numbers.

This means that for this issue, the corrupt check is only reliable during etcd start just after a crash.

## Impact

We are not aware of any cases of users reporting data corruption in production environments.

However, the issue was critical enough to motivate a public statement.
The main impact comes from losing user trust in etcd reliability.

## Lessons learned

### What went well

- Multiple maintainers were able to work effectively on reproducing and fixing the issue. As they are in different timezones, there was always someone working on the issue.
- When fixing the main data inconsistency we found multiple other edge cases that could lead to data corruption (https://github.com/etcd-io/etcd/issues/13514, https://github.com/etcd-io/etcd/issues/13922, https://github.com/etcd-io/etcd/issues/13937).

### What went wrong

- No users enabled data corruption detection as it is still an experimental feature introduced in v3.3. All reported cases were detected manually, making them almost impossible to reproduce.
- etcd has functional tests designed to detect such problems; however, they are unmaintained, flaky, and missing crucial scenarios.
- the etcd v3.5 release was not qualified as comprehensively as previous ones. Older maintainers ran a manual qualification process that is no longer documented or executed.
- The etcd apply code is complex; fixing the data inconsistency took almost two weeks and multiple attempts. The fix required additional automatic validation (https://github.com/etcd-io/etcd/pull/13885).
- etcd v3.5 was recommended for production without sufficient insight on production adoption. Production-ready recommendations were based on limited internal feedback and did not capture diverse usage patterns.

### Where we got lucky

- We reproduced the issue using functional tests only because of a non-standard partition setup on a workstation. Functional tests store etcd data under `/tmp`, which is usually mounted to an in-memory filesystem. The problem was reproduced only because one of the maintainers had `/tmp` mounted to a standard disk.

## Action items

Action items should directly address the items listed in lessons learned.
We should double down on things that went well, fix things that went wrong, and stop depending on luck.

Action items fall under three types, and we should have at least one item per type. Types:

- Prevent - Prevent similar issues from occurring. In this case, what testing we should introduce to find data inconsistency issues before release, preventing publishing broken release.
- Detect - Be more effective in detecting when similar issues occur. In this case, improve mechanism to detect data inconsistency issue so users will be automatically informed.
- Mitigate - Reduce time to recovery for users. In this case, how we ensure that users are able to quickly fix data inconsistency.

Actions should not be restricted to fixing the immediate issues and also propose long term strategic improvements.
To reflect this action items should have assigned priority:

- P0 - Critical for reliability of the v3.5 release. Should be prioritized this over all other work and backported to v3.5.
- P1 - Important for long term success of the project. Blocks v3.6 release.
- P2 - Stretch goals that would be nice to have for v3.6, however should not be blocking.

| Action Item                                                                         | Type     | Priority | Bug                                          | Status |
| ----------------------------------------------------------------------------------- | -------- | -------- | -------------------------------------------- | ------ |
| etcd testing can reproduce historical data inconsistency issues                     | Prevent  | P0       | https://github.com/etcd-io/etcd/issues/14045 | DONE   |
| etcd detects data corruption by default                                             | Detect   | P0       | https://github.com/etcd-io/etcd/issues/14039 | DONE   |
| etcd testing is high quality, easy to maintain and expand                           | Prevent  | P1       | https://github.com/etcd-io/etcd/issues/13637 |        |
| etcd apply code should be easy to understand and validate correctness               | Prevent  | P1       |                                              |        |
| Critical etcd features are not abandoned when contributors move on                  | Prevent  | P1       | https://github.com/etcd-io/etcd/issues/13775 | DONE   |
| etcd is continuously qualified with failure injection                               | Prevent  | P1       | https://github.com/etcd-io/etcd/pull/14911   | DONE   |
| etcd can reliably detect data corruption (hash is linearizable)                     | Detect   | P1       |                                              |        |
| etcd checks consistency of snapshots sent between leader and followers              | Detect   | P1       | https://github.com/etcd-io/etcd/issues/13973 | DONE   |
| etcd recovery from data inconsistency procedures are documented and tested          | Mitigate | P1       |                                              |        |
| etcd can imminently detect and recover from data corruption (implement Merkle root) | Mitigate | P2       | https://github.com/etcd-io/etcd/issues/13839 |        |

## Timeline

| Date       | Event                                                                                                                 |
| ---------- | --------------------------------------------------------------------------------------------------------------------- |
| 2021-05-08 | Pull request that caused data corruption was merged - https://github.com/etcd-io/etcd/pull/12855                      |
| 2021-06-16 | Release v3.5.0 with data corruption was published - https://github.com/etcd-io/etcd/releases/tag/v3.5.0               |
| 2021-12-01 | Report of data corruption - https://github.com/etcd-io/etcd/issues/13514                                              |
| 2021-01-28 | Report of data corruption - https://github.com/etcd-io/etcd/issues/13654                                              |
| 2022-03-08 | Report of data corruption - https://github.com/etcd-io/etcd/issues/13766                                              |
| 2022-03-25 | Corruption confirmed by one of the maintainers - https://github.com/etcd-io/etcd/issues/13766#issuecomment-1078897588 |
| 2022-03-29 | Statement about the corruption was sent to etcd-dev@googlegroups.com and dev@kubernetes.io                            |
| 2022-04-24 | Release v3.5.3 with fix was published - https://github.com/etcd-io/etcd/releases/tag/v3.5.3                           |
